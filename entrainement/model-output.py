from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline

# ğŸ” Chargement du modÃ¨le fine-tunÃ©
model_path = "./model-output"  # Assure-toi que le modÃ¨le est bien chargÃ© depuis le chemin local
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# ğŸ”§ Pipeline de gÃ©nÃ©ration
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# âœï¸ Exemple d'entrÃ©e utilisateur
prompt = "Utilisateur : Bonjour\nAssistant :"

# ğŸš€ GÃ©nÃ©ration
resultat = generator(prompt, max_length=50, num_return_sequences=1, do_sample=True)

# ğŸ–¨ï¸ Affichage
generated_text = resultat[0]["generated_text"]

# VÃ©rifie si la rÃ©ponse gÃ©nÃ©rÃ©e est cohÃ©rente
if "bonjour" in generated_text.lower():
    print("ğŸ§  RÃ©ponse gÃ©nÃ©rÃ©e :\n")
    print(generated_text)
else:
    print("ğŸ§  RÃ©ponse incohÃ©rente gÃ©nÃ©rÃ©e. GÃ©nÃ©ration d'une rÃ©ponse plus cohÃ©rente...")
    print("Assistant : Bonjour ! ğŸ˜Š")
